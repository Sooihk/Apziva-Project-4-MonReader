{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245ab804",
   "metadata": {},
   "source": [
    "# MonReader\n",
    "\n",
    "Our company develops innovative Artificial Intelligence and Computer Vision solutions that revolutionize industries. Machines that can see: We pack our solutions in small yet intelligent devices that can be easily integrated to your existing data flow. Computer vision for everyone: Our devices can recognize faces, estimate age and gender, classify clothing types and colors, identify everyday objects and detect motion. Technical consultancy: We help you identify use cases of artificial intelligence and computer vision in your industry. Artificial intelligence is the technology of today, not the future.\n",
    "\n",
    "MonReader is a new mobile document digitization experience for the blind, for researchers and for everyone else in need for fully automatic, highly fast and high-quality document scanning in bulk. It is composed of a mobile app and all the user needs to do is flip pages and everything is handled by MonReader: it detects page flips from low-resolution camera preview and takes a high-resolution picture of the document, recognizing its corners and crops it accordingly, and it dewarps the cropped document to obtain a bird's eye view, sharpens the contrast between the text and the background and finally recognizes the text with formatting kept intact, being further corrected by MonReader's ML powered redactor.\n",
    "\n",
    "The dataset was collected from page flipping video from smart phones and they was labelled as flipping and not flipping.\n",
    "The videos were clipped as short videos and was labelled as flipping or not flipping. The extracted frames are then saved to disk in a sequential order with the following naming structure: VideoID_FrameNumber\n",
    "\n",
    "* Goal(s):   \n",
    "Predict if the page is being flipped using a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30969c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#train_flip_dir = '../data/raw/training/flip'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf9e72",
   "metadata": {},
   "source": [
    "Here’s a guided tour of the code you’ve got in the canvas—what each chunk does and why it’s written that way. I’ll trace the three things you asked about: (1) how `ImageFolder` + `DataLoader` ingest your train/test folders, (2) how the **training** set is split into **train/20% validation** (stratified), and (3) how **VGG16** is built and trained in two phases.\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Reproducibility & config\n",
    "\n",
    "* `set_seed(...)` locks down Python/Torch (and NumPy if present) plus CuDNN determinism so the split and training are repeatable.\n",
    "* `Config` holds hyperparameters (classes, image size, batch size, workers) and—crucially—the two folder paths:\n",
    "\n",
    "  * `TRAIN_DIR = \"/path/to/dataset/training\"`\n",
    "  * `TEST_DIR  = \"/path/to/dataset/testing\"`\n",
    "\n",
    "Folder layout expected:\n",
    "\n",
    "```\n",
    "training/\n",
    "  flip/\n",
    "  notflip/\n",
    "testing/\n",
    "  flip/\n",
    "  notflip/\n",
    "```\n",
    "\n",
    "`ImageFolder` infers the class label from subfolder names (`flip` and `notflip`), mapping them to integer ids in `class_to_idx`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Transforms (ImageNet normalization is key)\n",
    "\n",
    "Two transform pipelines:\n",
    "\n",
    "* `train_transforms`: random, label-safe augmentation for generalization\n",
    "\n",
    "  * `Resize(int(224*1.14))` then `RandomResizedCrop(224)`: standard ImageNet-ish policy\n",
    "  * `RandomRotation(8)`, `ColorJitter(...)`: small perturbations\n",
    "  * `Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])`: **must** match ImageNet or the pretrained filters won’t “see” properly\n",
    "\n",
    "* `eval_transforms`: deterministic for validation/test\n",
    "\n",
    "  * `Resize` → `CenterCrop` → `Normalize`\n",
    "\n",
    "Because your label is literally about orientation (flip vs notflip), the code **avoids** horizontal/vertical flips in training augs (they’d change the class).\n",
    "\n",
    "---\n",
    "\n",
    "## 2) ImageFolder ingestion + stratified 80/20 split\n",
    "\n",
    "### Build two ImageFolder views on the **same training directory**\n",
    "\n",
    "```python\n",
    "train_full_eval = ImageFolder(TRAIN_DIR, transform=eval_transforms)\n",
    "train_full_aug  = ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "```\n",
    "\n",
    "Why two? You want **the same files**, but with different transform policies:\n",
    "\n",
    "* the **train** subset uses augmentations (`train_full_aug`)\n",
    "* the **val** subset uses deterministic transforms (`train_full_eval`)\n",
    "\n",
    "### Pull labels and make a stratified split\n",
    "\n",
    "```python\n",
    "all_targets = train_full_eval.targets\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=seed)\n",
    "train_idx, val_idx = next(splitter.split(range(len(all_targets)), all_targets))\n",
    "```\n",
    "\n",
    "* `ImageFolder.targets` is a list of class ids (e.g., 0 for `flip`, 1 for `notflip`).\n",
    "* `StratifiedShuffleSplit` guarantees the 80/20 split preserves the **class ratio** (your dataset is ~49/51 already).\n",
    "\n",
    "### Apply the **same index split** to both views via `Subset`\n",
    "\n",
    "```python\n",
    "train_ds = Subset(train_full_aug, train_idx)   # training with augs\n",
    "val_ds   = Subset(train_full_eval, val_idx)    # validation, no randomness\n",
    "```\n",
    "\n",
    "This pattern ensures consistency: you’re selecting the same filenames for train vs val, only changing the transform policy.\n",
    "\n",
    "### Testing dataset\n",
    "\n",
    "```python\n",
    "test_ds = ImageFolder(TEST_DIR, transform=eval_transforms)\n",
    "```\n",
    "\n",
    "Testing images get the deterministic pipeline; no augmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) DataLoaders for IO, batching, and shuffling\n",
    "\n",
    "```python\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  ...)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, ...)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, ...)\n",
    "```\n",
    "\n",
    "* Training loader shuffles each epoch (good!).\n",
    "* Validation/test loaders don’t shuffle, giving stable evaluation.\n",
    "* `num_workers` + `pin_memory` speed up host→GPU transfers.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Build VGG16 for binary classification\n",
    "\n",
    "### Load pretrained backbone\n",
    "\n",
    "```python\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "```\n",
    "\n",
    "This pulls in the ImageNet-trained filters (the “visual grammar”).\n",
    "\n",
    "### Replace the classifier head\n",
    "\n",
    "The script uses a **lighter**, two-layer head (recommended for small, binary tasks):\n",
    "\n",
    "```python\n",
    "vgg.classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512*7*7, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, 2)   # flip vs notflip\n",
    ")\n",
    "```\n",
    "\n",
    "* VGG16’s feature extractor outputs `512x7x7` for 224×224 inputs → flatten to 25088.\n",
    "* Smaller head = fewer parameters and less overfitting than the giant 4096→4096 original.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Training in two phases (best practice for transfer learning)\n",
    "\n",
    "### Phase 1: “Linear probe”\n",
    "\n",
    "```python\n",
    "for p in vgg.features.parameters():\n",
    "    p.requires_grad = False\n",
    "optimizer = AdamW(head_params, lr=1e-3, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "* **Freeze** all convolutional blocks → only train your new classifier head.\n",
    "* Use a higher LR for the randomly initialized head.\n",
    "* Train a few epochs; this teaches the head to map the already-good features to your two classes.\n",
    "\n",
    "### Phase 2: Gradual unfreeze (fine-tune the highest-level features)\n",
    "\n",
    "```python\n",
    "for p in vgg.features[-10:].parameters():   # roughly Block 5\n",
    "    p.requires_grad = True\n",
    "\n",
    "optimizer = AdamW([\n",
    "  {\"params\": backbone_params, \"lr\": 2e-5},  # tiny LR for pretrained layers\n",
    "  {\"params\": head_params,     \"lr\": 1e-4},  # bigger LR for head\n",
    "], weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "* Unfreeze the **last conv block** (Block 5) so it can adapt from “ImageNet semantics” to “orientation semantics.”\n",
    "* Use **discriminative LRs**: small for backbone, larger for the head, to avoid wrecking pretrained filters.\n",
    "* Keep best validation checkpoint.\n",
    "\n",
    "### AMP for speed\n",
    "\n",
    "`autocast` + `GradScaler` turn on mixed precision automatically when CUDA is available—nice training speedup with stable numerics.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Evaluation helpers\n",
    "\n",
    "* `run_one_epoch(...)` handles a full pass (train or eval) and returns average loss + accuracy.\n",
    "* `validate(...)` just calls the same loop with `no_grad()` and eval mode.\n",
    "* There’s a **test evaluation** function you run **once at the end** (after model selection and tuning are done) to get a clean, unbiased final score.\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR (the flow in one breath)\n",
    "\n",
    "* Read folders with `ImageFolder` → pull labels → **stratified 80/20 split** using indices.\n",
    "* Wrap those subsets in `DataLoader`s (train shuffles, val/test don’t).\n",
    "* Load **ImageNet-pretrained VGG16**, swap in a small **2-class** head.\n",
    "* **Phase 1:** freeze conv layers and train only the head.\n",
    "* **Phase 2:** unfreeze the last conv block and fine-tune with a small LR.\n",
    "* Track validation, save best checkpoint, and only then run the **test** evaluation once.\n",
    "\n",
    "If you want, I can annotate the canvas file inline with numbered comments next to each section, or clone the same scaffold for ResNet/EfficientNet/MobileNet so all four models share transforms, loaders, and training schedules for a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edca838",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms, models\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# For a clean, stratified split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StratifiedShuffleSplit\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# libraries to import\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# For a clean, stratified split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible function\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set seeds for python, numpy (if used), and torch to have reproducible splits/training.\"\"\"\n",
    "    random.seed(seed)\n",
    "    try:\n",
    "        import numpy as np\n",
    "        np.random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Make CuDNN deterministic (slightly slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # shape final classifier to flip vs notflip\n",
    "    num_classes: int = 2\n",
    "    img_size: int = 224               # VGG16 default input size\n",
    "    # pass to every DataLoader to set how many images per step\n",
    "    batch_size: int = 32\n",
    "    # set default amount of workers (background processes to load/augment data per DataLoader)\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "    seed: int = 42 # choosing seed for reproducible results\n",
    "\n",
    "    # file system pointers to the training and testing images, fed into torchvision.datatsets.ImageFolder\n",
    "    # training root containing one subfolder per class\n",
    "    TRAIN_DIR: str = \"/path/to/dataset/training\"\n",
    "    # held out test root, not touched during training/hyperparameter tuning\n",
    "    TEST_DIR: str  = \"/path/to/dataset/testing\"\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fe267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two transform pipelines \n",
    "#   - training (with light augmentation)\n",
    "#   - validation/testing \n",
    "# VGG16 was pretrained on ImageNet with these stats.\n",
    "# Keeping these values preserves the meaning of its filters.\n",
    "# RGB mean/std used when VGG16 was trained on ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\"\"\"\n",
    "    # A light set of augmentations to improve generalization while preserving label semantics.\n",
    "    # 1. scale short side from 224 to 256\n",
    "    # 2. Randomly choooses a crop then resizes that crop to 224x224, in order to train model to be robust to framing/zoom changes\n",
    "    # 3. rotates image up to 8 degrees, small label preserving perturbation\n",
    "    # 4 ColorJitter for mild brightness/contrast/saturation/hue changes to help resist lightning and white balance quirks\n",
    "    # from phone cameras.\n",
    "    # 5. Converts a PIL image to a PyTorch tensor shaped [C,H,W]\n",
    "    # 6. Final conversion and ImageNet centering \n",
    "\n",
    "    Net effect is a moderate, label-safe augmentation \n",
    "    \"\"\"\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(int(cfg.img_size * 1.14)),    # 1      \n",
    "    transforms.RandomResizedCrop(cfg.img_size),     # 2          \n",
    "    transforms.RandomRotation(8),   # 3    \n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),    # 4\n",
    "    transforms.ToTensor(),      # 5\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),      # 6 \n",
    "    ])\n",
    "\n",
    "# Validation/Test should be deterministic and comparable across epochs\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize(int(cfg.img_size * 1.14)),\n",
    "    transforms.CenterCrop(cfg.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5045654",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We instantiate TWO ImageFolder objects pointing to the SAME training directory:\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#   - one with train_transforms / training (random augments)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#   - one with eval_transforms / validation (deterministic transforms)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# create ImageFolder that reads file path under TRAIN_DIR and apply determinstic transforms (resize, center, crop, normalize)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_full_eval = \u001b[43mdatasets\u001b[49m.ImageFolder(cfg.TRAIN_DIR, transform=eval_transforms)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# create 2nd ImageFolder over the same directory but apply random augmentations (crop,rotation,jitter) for training\u001b[39;00m\n\u001b[32m     10\u001b[39m train_full_aug  = datasets.ImageFolder(cfg.TRAIN_DIR, transform=train_transforms)\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# We instantiate TWO ImageFolder objects pointing to the SAME training directory:\n",
    "#   - one with train_transforms / training (random augments)\n",
    "#   - one with eval_transforms / validation (deterministic transforms)\n",
    "# Then we apply the SAME index split (train_idx / val_idx) to both via Subset.\n",
    "# This pattern keeps transform policies separate while reusing the same file list.\n",
    "\n",
    "# create ImageFolder that reads file path under TRAIN_DIR and apply determinstic transforms (resize, center, crop, normalize)\n",
    "train_full_eval = datasets.ImageFolder(cfg.TRAIN_DIR, transform=eval_transforms)\n",
    "# create 2nd ImageFolder over the same directory but apply random augmentations (crop,rotation,jitter) for training\n",
    "train_full_aug  = datasets.ImageFolder(cfg.TRAIN_DIR, transform=train_transforms)\n",
    "\n",
    "# grab numeric labels for evert image to feed stratified splitter \n",
    "all_targets: List[int] = train_full_eval.targets\n",
    "\n",
    "# make a stratified 80/20 split on all_targets indicies\n",
    "# Stratified split: hold out 20% of training images as validation while preserving class ratios\n",
    "split_ratio = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=cfg.seed)\n",
    "train_idx, val_idx = next(split_ratio.split(X=range(len(all_targets)), y=all_targets))\n",
    "\n",
    "# wrap the augmented dataset in a subset that exposes only the training indicies \n",
    "# Result is a training dataset that applies random transform on the training images. \n",
    "training_dataset = Subset(train_full_aug, train_idx)\n",
    "\n",
    "# Do the same wrap on validation \n",
    "validation_dataset   = Subset(train_full_eval, val_idx) \n",
    "\n",
    "# Build the final testing dataset (never augmented)\n",
    "# Testing directory is separate and already labeled by subfolders\n",
    "test_ds = datasets.ImageFolder(cfg.TEST_DIR, transform=eval_transforms)\n",
    "\n",
    "# check to see how many images ended up in each split \n",
    "print(f\"Training images: {len(training_dataset)} | Validation Images: {len(validation_dataset)} | Testing Images: {len(test_ds)}\")\n",
    "# check to see class names and numeric mapping\n",
    "print(f\"Classes: {train_full_eval.classes} (class_to_idx={train_full_eval.class_to_idx})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e689021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here I implement DataLoader which turns my Datasets into mini-batches, shuffles them,\n",
    "and uses background worker processes to load/transform images in parallel.\n",
    "Batching: Packs single (image, label) samples into [B,C,H,W] tensors so model trains in vectorized chunks\n",
    "Shuffling: Randomizes sample order for train only each epoch to stablize SGD and reduce bias from data order\n",
    "Data plumbing, feeding the right split (train/val/test) to the model with the right behavior (augment+shuffle for train; deterministic for eval)\n",
    "\"\"\"\n",
    "# DataLoader handles batching, shuffling (for training), and parallel disk I/O.\n",
    "train_loader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,                 # shuffle only for training\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,      # False for reproducible and comparable metrics across runs\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VGG16 backbone and adapt it for 2-class output\n",
    "# Simple, stacked 3x3 conv blocks → predictable behavior & easy fine-tuning\n",
    "\n",
    "# Load ImageNet-pretrained VGG16, transfer learning\n",
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Option for binary tasks: Replace the whole classifier with a lighter head to reduce parameters and improve generalization.\n",
    "# VGG16 feature extractor ends with 512 feature maps at spatial size 7x7 (for 224x224 inputs).\n",
    "# Flattened dimension = 512 * 7 * 7 = 25088.\n",
    "# Original head 25088-4096-4096-1000 with ReLUs & Dropout, high overfitting risk on this small dataset\n",
    "# choosing new head: 25088-512-2 with ReLU & Dropout, with binary task and limited data I dont need the original 120M classifier parameters.\n",
    "# Backbone already encodes rich features, a compact head is enough to separate two classes.\n",
    "\n",
    "vgg.classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512 * 7 * 7, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, cfg.num_classes)\n",
    ")\n",
    "# moves weights to GPU if available else CPU\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Phase 1: Linear Probe\n",
    "# Freeze the convolutional blocks and train only the new classifier head to see quickly if this head\n",
    "# can separate classes with generic ImageNet features. \n",
    "# Freeze the backbone by telling autograd not to store gradients for any parameters inside vgg.features\n",
    "for parameter in vgg.features.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Optimizer & loss: use a higher LR for the head since it's randomly initialized\n",
    "# As backbone's requires_grad=False, filter out non frozen ones and keep trainiable params (classifier head)\n",
    "head_params = [param for param in vgg.parameters() if param.requires_grad]\n",
    "\n",
    "# only head's parameters are optimized with AdamW, with higher LR as head is randomly initialized and needs to move quickly to good region\n",
    "# Later during fine tuning, use param groups to give backbone tiny LR and larger LR for head\n",
    "optimizer = torch.optim.AdamW(head_params, lr=1e-3, weight_decay=1e-4)\n",
    "# multi class classification loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\"\"\" \n",
    "Code walkthrough during training:\n",
    "1. Forward: images > frozen backbone > features > trainable head > logits.\n",
    "2. Loss: criterion(logits, targets) computes scalar loss\n",
    "3.\tBackward: only head params accumulate gradients (backbone params don’t).\n",
    "4.\tStep: optimizer.step() updates the head only.\n",
    "5.\tResult: Learn a linear/nonlinear separator on top of fixed features—fast and low-variance.\n",
    "\"\"\"\n",
    "\n",
    "# Mixed-precision speeds up training on GPUs without numeric instability.\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd44a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two helper functions for training and validating. \n",
    "\n",
    "# function for training to do a full pass over a DataLoader.\n",
    "# Runs model forward, compute loss, backdrop + optimizer step, and aggregate avg loss and accuracy\n",
    "def run_one_epoch(model: nn.Module, loader: DataLoader, train: bool=True) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Process one full pass over a dataset split (training or validation) and returns metrics. \n",
    "    1. Sets the mode, toggle between training / evaluation\n",
    "    2. Iterates batches, pulls image/label from DataLoader\n",
    "    3. Moves to device\n",
    "    4. Forward pass (optionally AMP under autocast), computes logits and loss\n",
    "    5. Optimizes if and only if training\n",
    "        - zeros gradients, backprops with loss scaling, steps the optimizer via the scaler\n",
    "    6. Keeps running sum of losses and correct predictions, then computes avg loss and accuracy over entire epoch.\n",
    "    Returns (avg_loss, accuracy).\n",
    "    \"\"\"\n",
    "    # Toggle model's mode, if True enable training mode: Dropout active, BatchNorm uses batch stats.\n",
    "    # If False eval mode: Dropout off, BatchNorm uses running stats.\n",
    "    model.train(train)\n",
    "\n",
    "    # Initialize counters for dataset metrics\n",
    "    total, correct_pred, loss_sum = 0,0,0.0\n",
    "    # Iterate over the DataLoader, each iteration yields a mini batch\n",
    "    for image, label in loader:\n",
    "        image, label = image.to(device), label.to(device)   # move tensors to selected device\n",
    "\n",
    "        # if CUDA avaiable\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(image)   # forward pass\n",
    "            loss = criterion(logits, label) # compute CrossEntropyLoss \n",
    "        # Only do optimizer during training\n",
    "        if train:\n",
    "            # clear old gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # backward pass with loss scaling (AMP)\n",
    "            scaler.scale(loss).backward() \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update() # adjust scaling factor for the next iteration\n",
    "        # accumlate sum of losses over examples\n",
    "        loss_sum += loss.item() * label.size(0)\n",
    "        predictions = logits.argmax(dim=1) # argmax over class dimension to get prediced class indices\n",
    "        # count how many predictions matched ground truth in this batch\n",
    "        correct_pred += (predictions == label).sum().item()\n",
    "        total += label.size(0)\n",
    "    # return avg loss and accuracy\n",
    "    return loss_sum / total, correct_pred / total\n",
    "\n",
    "# Switch model to eval mode, disable gradient tracking \n",
    "def validate(model: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval() # put model in evaluation mode\n",
    "    # Disable gradient tracking\n",
    "    with torch.no_grad():\n",
    "        # run run_one_epoch func using validation loader\n",
    "        return run_one_epoch(model, val_loader, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3250a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Linear-probe Warmup\n",
    "# Keep VGG backbone frozen and train only new classifer head after a few epochs. \n",
    "# Goal: quickly learn a decision boundary on top of the generic ImageNet features, check that learning\n",
    "# is happening and get a stable point before unfreezing conv layers. \n",
    "\"\"\" \n",
    "Reasons: \n",
    "- Training only the small head first avoids blasting the pretrained conv filters with large random gradients.\n",
    "- Fewer trainable parameters leads to a quick convergence signal. \n",
    "- Diagnostics: If the head can't reach decent validation accuracy with the backbone frozen, then the issue is likely transforms\n",
    "/labels/splits not fine-tuning. \n",
    "\"\"\"\n",
    "EPOCHS_HEAD = 5 # short warmp to train new classifier head\n",
    "for epoch in range(EPOCHS_HEAD):\n",
    "    training_loss, training_accuracy = run_one_epoch(vgg, train_loader, train=True)\n",
    "    validation_loss, validation_accuracy = validate(vgg)\n",
    "    print(f\"[Head] Epoch {epoch+1:02d} | Training Accuracy: {training_acccuracy:.4f} | Validation Accuracy: {validation_accuracy:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6177cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: unfreeze last conv block(s) and fine tune. \n",
    "\"\"\" \n",
    "After linear probe, start to unfreeze small part of the ImageNet backbone and fine tune gently with small LR. \n",
    "Adapt high-level ImageNet features to my (flip vs notflip) specific task without wrecking the useful low-level\n",
    "filters the model already learned. \n",
    "\"\"\"\n",
    "\n",
    "# Turn training on roughly for the last VGG conv block (5). The deepest conv layers encode task-specific abstractions \n",
    "# These layers benefit the most from fine-tuning, earliers left frozen to avoid overfitting\n",
    "for layer in vgg.features[-10:].parameters():\n",
    "    layer.requires_grad = True\n",
    "\n",
    "# Give different learning rates, a smaller LR for the backbone and a higher LR for the head classifier\n",
    "backbone_parameters = [layer for layer in vgg.features.parameters() if layer.requires_grad]\n",
    "head_parameters = [layer for layer in vgg.classifier.parameters() if layer.requires_grad]\n",
    "\n",
    "# Rebuild the optimizer with the two different LRs\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': backbone_parameters, 'lr':2e-5}, # small LR for the pretrained conv\n",
    "    {'params': head_parameters, 'lr': 1e-4},\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# Fine tune for 10 epochs, slighly longer than head-only warmup\n",
    "epochs_fine_tuning = 10\n",
    "# Intialize varianles to track best validation accuracy and the corresponding model checkpoint\n",
    "best_validation_accuracy = 0.0\n",
    "best_checkpoint = None\n",
    "\n",
    "# standard train and validiation loop with partially unfrozen model\n",
    "for epoch in range(epochs_fine_tuning): \n",
    "    training_loss, training_accuracy = run_one_epoch(vgg, train_loader, train=True)\n",
    "    validation_loss, validation_accuracy = validate(vgg)\n",
    "    print(f\"[Fine Tuning] Epoch {epoch+1:02d} | Training Accuracy {training_accuracy:.4f} | Validation Accuracy {validation_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model by validation accuracy\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "        best_checkpoint = {k: v for k, v in vgg.state_dict().items()}\n",
    "\n",
    "\n",
    "\n",
    "# Restore best val checkpoint (if any)\n",
    "if best_state is not None:\n",
    "    vgg.load_state_dict(best_state)\n",
    "    print(f\"Restored best model with Val Acc = {best_val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VGG16 Transfer Learning Pipeline (PyTorch)\n",
    "-----------------------------------------\n",
    "This script shows how to:\n",
    "  1) Load folder-structured image data using `ImageFolder` + `DataLoader`.\n",
    "  2) Split the training set into Train / Validation (20% val) **stratified** by class.\n",
    "  3) Build a VGG16 model with ImageNet weights and adapt it for a binary task.\n",
    "  4) (Optional) Train in two phases: linear probe (freeze backbone) and fine-tune.\n",
    "\n",
    "Folder structure expected (example):\n",
    "  dataset_root/\n",
    "    training/\n",
    "      flip/\n",
    "        img001.jpg\n",
    "        ...\n",
    "      notflip/\n",
    "        imgXYZ.jpg\n",
    "        ...\n",
    "    testing/\n",
    "      flip/\n",
    "        ...\n",
    "      notflip/\n",
    "        ...\n",
    "\n",
    "Replace the `TRAIN_DIR` and `TEST_DIR` paths below with your actual paths.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# For a clean, stratified split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Reproducibility helpers\n",
    "# ---------------------------\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set seeds for python, numpy (if used), and torch to have reproducible splits/training.\"\"\"\n",
    "    random.seed(seed)\n",
    "    try:\n",
    "        import numpy as np\n",
    "        np.random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Make CuDNN deterministic (slightly slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Config\n",
    "# ---------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    num_classes: int = 2               # flip vs notflip\n",
    "    img_size: int = 224               # VGG16 default input size\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "    pin_memory: bool = True\n",
    "    seed: int = 42\n",
    "\n",
    "    # file system pointers to the training and testing images, fed into torchvision.datatsets.ImageFolder\n",
    "    # training root containing one subfolder per class\n",
    "    TRAIN_DIR: str = \"/path/to/dataset/training\"\n",
    "    # held out test root, not touched during training/hyperparameter tuning\n",
    "    TEST_DIR: str  = \"/path/to/dataset/testing\"\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) Define transforms (ImageNet normalization)\n",
    "# ---------------------------------------------\n",
    "# IMPORTANT: VGG16 was pretrained on ImageNet with these stats.\n",
    "# Keeping them preserves the meaning of its filters.\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # A light set of augmentations to improve generalization while preserving label semantics.\n",
    "    transforms.Resize(int(cfg.img_size * 1.14)),          # resize short side similarly to torchvision presets\n",
    "    transforms.RandomResizedCrop(cfg.img_size),            # random crop to target size\n",
    "    transforms.RandomRotation(8),                          # SMALL rotations; avoid flips since your label encodes orientation\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),           # mild color/brightness changes\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Validation/Test should be deterministic and comparable across epochs\n",
    "eval_transforms = transforms.Compose([\n",
    "    transforms.Resize(int(cfg.img_size * 1.14)),\n",
    "    transforms.CenterCrop(cfg.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) Build datasets with ImageFolder and make a stratified train/val split\n",
    "# --------------------------------------------------------------------\n",
    "# We instantiate TWO ImageFolder objects pointing to the SAME training directory:\n",
    "#   - one with train_transforms\n",
    "#   - one with eval_transforms\n",
    "# Then we apply the SAME index split (train_idx / val_idx) to both via Subset.\n",
    "# This pattern keeps transform policies separate while reusing the same file list.\n",
    "\n",
    "train_full_eval = datasets.ImageFolder(cfg.TRAIN_DIR, transform=eval_transforms)\n",
    "train_full_aug  = datasets.ImageFolder(cfg.TRAIN_DIR, transform=train_transforms)\n",
    "\n",
    "# The targets are accessible on ImageFolder via `.targets` (a list of int class labels per image)\n",
    "all_targets: List[int] = train_full_eval.targets\n",
    "\n",
    "# Stratified split: hold out 20% of training images as validation while preserving class ratios\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=cfg.seed)\n",
    "train_idx, val_idx = next(splitter.split(X=range(len(all_targets)), y=all_targets))\n",
    "\n",
    "# Subset the two datasets with identical indices\n",
    "train_ds = Subset(train_full_aug, train_idx)  # augmentations enabled\n",
    "val_ds   = Subset(train_full_eval, val_idx)    # eval transforms (no random aug)\n",
    "\n",
    "# Build the final testing dataset (never augmented)\n",
    "# Testing directory is separate and already labeled by subfolders\n",
    "test_ds = datasets.ImageFolder(cfg.TEST_DIR, transform=eval_transforms)\n",
    "\n",
    "print(f\"Train images: {len(train_ds)} | Val images: {len(val_ds)} | Test images: {len(test_ds)}\")\n",
    "print(f\"Classes: {train_full_eval.classes} (class_to_idx={train_full_eval.class_to_idx})\")\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 4) Create DataLoaders for each split\n",
    "# -------------------------------------\n",
    "# DataLoader handles batching, shuffling (for training), and parallel disk I/O.\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,                 # shuffle only for training\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=cfg.pin_memory,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Build VGG16 backbone and adapt it for 2-class output\n",
    "# ------------------------------------------------------\n",
    "# Why VGG16?\n",
    "# - Simple, stacked 3x3 conv blocks → predictable behavior & easy fine-tuning\n",
    "# - Strong ImageNet-pretrained features for edges/textures/parts\n",
    "\n",
    "# Load ImageNet-pretrained VGG16\n",
    "vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Option A: Keep the original (big) classifier and just replace the last layer.\n",
    "#   Pros: quick setup, strong capacity. Cons: very large parameter count (can overfit small datasets).\n",
    "# vgg.classifier[-1] = nn.Linear(vgg.classifier[-1].in_features, cfg.num_classes)\n",
    "\n",
    "# Option B (RECOMMENDED for binary tasks): Replace the whole classifier with a lighter head\n",
    "#   to reduce parameters and improve generalization.\n",
    "# VGG16 feature extractor ends with 512 feature maps at spatial size 7x7 (for 224x224 inputs).\n",
    "# Flattened dimension = 512 * 7 * 7 = 25088.\n",
    "\n",
    "vgg.classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(512 * 7 * 7, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(512, cfg.num_classes)\n",
    ")\n",
    "\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "# Phase 1: \"Linear probe\" — freeze convolutional blocks and train only the new classifier head.\n",
    "for p in vgg.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Optimizer & loss: use a higher LR for the head since it's randomly initialized\n",
    "head_params = [p for p in vgg.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(head_params, lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# (Optional) Mixed-precision speeds up training on GPUs without numeric instability.\n",
    "use_amp = torch.cuda.is_available()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6) Training & evaluation helper routines\n",
    "# ---------------------------------------\n",
    "\n",
    "def run_one_epoch(model: nn.Module, loader: DataLoader, train: bool = True) -> Tuple[float, float]:\n",
    "    \"\"\"Run a single epoch over a DataLoader.\n",
    "    Returns (avg_loss, accuracy).\n",
    "    \"\"\"\n",
    "    model.train(train)\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Stats\n",
    "        loss_sum += loss.item() * targets.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "def validate(model: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return run_one_epoch(model, val_loader, train=False)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# 7) Phase 1 training: head only (few epochs)\n",
    "# -------------------------------------\n",
    "EPOCHS_HEAD = 5  # short warmup to train the new classifier head\n",
    "for epoch in range(EPOCHS_HEAD):\n",
    "    tr_loss, tr_acc = run_one_epoch(vgg, train_loader, train=True)\n",
    "    va_loss, va_acc = validate(vgg)\n",
    "    print(f\"[Head] Epoch {epoch+1:02d} | Train Acc {tr_acc:.4f} | Val Acc {va_acc:.4f}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 8) Phase 2: unfreeze last conv block(s) and fine-tune\n",
    "# --------------------------------------------------------\n",
    "# Strategy: unfreeze the deepest block (Block 5) first. Use a **smaller** LR so we gently\n",
    "# adapt high-level features from ImageNet to your specific task (flip vs notflip).\n",
    "\n",
    "# VGG16 features layout (for reference):\n",
    "# Blocks are contiguous in vgg.features; last ~10 layers correspond roughly to Block 5.\n",
    "for p in vgg.features[-10:].parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Use discriminative learning rates: smaller for backbone, higher for head\n",
    "backbone_params = [p for p in vgg.features.parameters() if p.requires_grad]\n",
    "head_params     = [p for p in vgg.classifier.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": backbone_params, \"lr\": 2e-5},   # tiny LR for pretrained conv layers\n",
    "    {\"params\": head_params,     \"lr\": 1e-4},   # slightly larger LR for classifier\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "EPOCHS_FT = 10\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(EPOCHS_FT):\n",
    "    tr_loss, tr_acc = run_one_epoch(vgg, train_loader, train=True)\n",
    "    va_loss, va_acc = validate(vgg)\n",
    "    print(f\"[FT ] Epoch {epoch+1:02d} | Train Acc {tr_acc:.4f} | Val Acc {va_acc:.4f}\")\n",
    "\n",
    "    # Save the best model by validation accuracy\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        best_state = {k: v for k, v in vgg.state_dict().items()}\n",
    "\n",
    "# Restore best val checkpoint (if any)\n",
    "if best_state is not None:\n",
    "    vgg.load_state_dict(best_state)\n",
    "    print(f\"Restored best model with Val Acc = {best_val_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 9) Final evaluation on the held-out test set\n",
    "# ---------------------------------------\n",
    "# NOTE: Only run this once you've finalized hyperparameters using the validation set.\n",
    "# This ensures the test performance is an unbiased estimate.\n",
    "\n",
    "def evaluate_on_test(model: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total, correct, loss_sum = 0, 0, 0.0\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss_sum += loss.item() * targets.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "        return loss_sum / total, correct / total\n",
    "\n",
    "# Example (comment out if you don't want to evaluate yet):\n",
    "# test_loss, test_acc = evaluate_on_test(vgg)\n",
    "# print(f\"[TEST] Loss {test_loss:.4f} | Acc {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 10) Tips and gotchas\n",
    "# ---------------------------------------\n",
    "# - Avoid RandomHorizontalFlip/VerticalFlip unless you also flip the LABEL\n",
    "#   (since your target is literally about flip vs notflip). That complicates pipelines.\n",
    "# - Monitor per-class metrics (precision/recall/F1) to ensure neither class lags.\n",
    "# - If you see overfitting:\n",
    "#     * Increase dropout in the classifier head\n",
    "#     * Add mild augmentations (ColorJitter, RandomPerspective)\n",
    "#     * Increase weight decay, or reduce LR\n",
    "# - For speed: use AMP (already enabled), increase num_workers if your disk is fast.\n",
    "# - For fairness across backbones (ResNet/EfficientNet/MobileNet): reuse the SAME transforms and schedules.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Pipeline built. Uncomment training calls to run end-to-end.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
